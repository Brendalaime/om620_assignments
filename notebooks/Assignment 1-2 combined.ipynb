{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba40ee6",
   "metadata": {},
   "source": [
    "# OM 620 — Milestone III: Inventory Analytics & Safety Stock Showcase\n",
    "**Student:** Brenda Laime  \n",
    "**Course:** OM 620 — Tools & Technologies for Analytics  \n",
    "**Dataset:** `transaction_data.csv` (same dataset as Assignments 1 & 2)\n",
    "\n",
    "This single notebook **combines Assignment 1 and Assignment 2** to showcase the full workflow:\n",
    "\n",
    "- **Assignment 1 (Data Cleaning & Filtering)**  \n",
    "  1) Standardize/clean columns and types  \n",
    "  2) Inspect numeric features, fix anomalies (returns, invalid prices)  \n",
    "  3) Handle missing values  \n",
    "  4) Filter to FG + MTS and answer basic questions\n",
    "\n",
    "- **Assignment 2 (Safety Stock)**  \n",
    "  1) Transform to per-SKU stats (min/max/avg/median/var/std, avg lead time)  \n",
    "  2) Compute safety stock at 75%, 90%, 95% service levels  \n",
    "  3) Report the largest/smallest/average safety stock (95%)  \n",
    "  4) **Side Quest**: Empirical (non-parametric) safety stock using 95th percentile\n",
    "\n",
    "> This notebook is **self-contained**: it reloads and cleans the raw data so it can be run independently. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea87ec1",
   "metadata": {},
   "source": [
    "## Assignment 1 — Data Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb09773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data (place this notebook under repo/notebooks and put CSV in repo/data)\n",
    "df = pd.read_csv(\"../data/transaction_data.csv\")\n",
    "\n",
    "# --- Q1: Dataframe formatting (standardize columns) ---\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "df.rename(columns={\"sku_numb3r\": \"sku_number\"}, inplace=True)\n",
    "\n",
    "# --- Q2: Inspection & anomaly checks ---\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "\n",
    "# returns flag for negative quantities\n",
    "df['is_return'] = df['order_quantity'] < 0\n",
    "\n",
    "# non-positive unit prices -> mark missing (then we fill)\n",
    "df.loc[df['unit_price'] <= 0, 'unit_price'] = np.nan\n",
    "\n",
    "print('Negative order quantities:', (df['order_quantity'] < 0).sum())\n",
    "print('Non-positive unit prices (now NaN):', df['unit_price'].isna().sum())\n",
    "\n",
    "# --- Q3: Handle NaNs ---\n",
    "fill_map = {\n",
    "    \"order_quantity\": 0,\n",
    "    \"unit_price\": 0,\n",
    "    \"sku_number\": \"Unknown\",\n",
    "    \"inventory_type\": \"Unknown\",\n",
    "    \"stocking_type\": \"Unknown\",\n",
    "    \"manufacturing_site\": \"Unknown\",\n",
    "    \"division_code\": \"Unknown\",\n",
    "}\n",
    "df = df.fillna(fill_map)\n",
    "\n",
    "# --- Q4: Useful information on FG + MTS subset ---\n",
    "fg_mts = df[(df[\"inventory_type\"] == \"FG\") & (df[\"stocking_type\"] == \"MTS\")].copy()\n",
    "\n",
    "# Add total_sales for reporting\n",
    "fg_mts['total_sales'] = fg_mts['order_quantity'] * fg_mts['unit_price']\n",
    "\n",
    "# Summary answers\n",
    "unique_skus = fg_mts['sku_number'].nunique()\n",
    "unique_sites = fg_mts['manufacturing_site'].nunique()\n",
    "unique_divs = fg_mts['division_code'].nunique()\n",
    "\n",
    "print('Unique SKUs:', unique_skus)\n",
    "print('Unique Manufacturing Sites:', unique_sites)\n",
    "print('Unique Division Codes:', unique_divs)\n",
    "\n",
    "# Preview top/bottom examples\n",
    "cols_show = [\"sku_number\",\"manufacturing_site\",\"division_code\",\"order_quantity\",\"unit_price\",\"total_sales\"]\n",
    "\n",
    "top10_qty = fg_mts.sort_values('order_quantity', ascending=False).head(10)[cols_show]\n",
    "bottom10_qty = fg_mts.sort_values('order_quantity', ascending=True).head(10)[cols_show]\n",
    "\n",
    "top10_sales = fg_mts.sort_values('total_sales', ascending=False).head(10)[cols_show]\n",
    "bottom10_sales = fg_mts.sort_values('total_sales', ascending=True).head(10)[cols_show]\n",
    "\n",
    "top10_qty.head(3), bottom10_qty.head(3), top10_sales.head(3), bottom10_sales.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c6997",
   "metadata": {},
   "source": [
    "**A1 notes (short):**  \n",
    "- I keep negative `order_quantity` as **returns** via `is_return`.  \n",
    "- I set invalid `unit_price` (≤ 0) to `NaN` and then fill with 0 (so later math runs).  \n",
    "- I filled missing text with `\"Unknown\"` to retain rows.  \n",
    "- Then I filtered to **Finished Goods + Make-To-Stock (FG + MTS)** for all downstream analysis.  \n",
    "- I added `total_sales = order_quantity × unit_price` for interpretable top/bottom tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1e946",
   "metadata": {},
   "source": [
    "## Assignment 2 — Safety Stock (Service Levels & Empirical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16462a36",
   "metadata": {},
   "source": [
    "### Q1 — Per-SKU Transformation\n",
    "For each SKU in the FG+MTS subset, compute: min, max, mean, median, variance, standard deviation of `order_quantity`, and average `lead_time`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c36e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_stats = (\n",
    "    fg_mts.groupby(\"sku_number\")\n",
    "    .agg(\n",
    "        min_qty = (\"order_quantity\", \"min\"),\n",
    "        max_qty = (\"order_quantity\", \"max\"),\n",
    "        avg_qty = (\"order_quantity\", \"mean\"),\n",
    "        median_qty = (\"order_quantity\", \"median\"),\n",
    "        var_qty = (\"order_quantity\", \"var\"),\n",
    "        std_qty = (\"order_quantity\", \"std\"),\n",
    "        avg_lead_time = (\"lead_time\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "sku_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb4503",
   "metadata": {},
   "source": [
    "### Q2 — Safety Stock @ 75%, 90%, 95%\n",
    "Formula: **SS = z × σ × √L**  \n",
    "- 75% → z ≈ 0.674  \n",
    "- 90% → z ≈ 1.282  \n",
    "- 95% → z ≈ 1.645  \n",
    "Where σ is `std_qty` and L is `avg_lead_time`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fedce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_75, z_90, z_95 = 0.674, 1.282, 1.645\n",
    "sku_stats['sqrt_lead_time'] = np.sqrt(sku_stats['avg_lead_time'])\n",
    "\n",
    "sku_stats['safety_stock_75'] = z_75 * sku_stats['std_qty'] * sku_stats['sqrt_lead_time']\n",
    "sku_stats['safety_stock_90'] = z_90 * sku_stats['std_qty'] * sku_stats['sqrt_lead_time']\n",
    "sku_stats['safety_stock_95'] = z_95 * sku_stats['std_qty'] * sku_stats['sqrt_lead_time']\n",
    "\n",
    "sku_stats[['safety_stock_75','safety_stock_90','safety_stock_95']] = (\n",
    "    sku_stats[['safety_stock_75','safety_stock_90','safety_stock_95']].fillna(0)\n",
    ")\n",
    "sku_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea413e",
   "metadata": {},
   "source": [
    "### Q3 — Largest / Smallest / Average (95% Service Level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_95 = sku_stats[['sku_number','safety_stock_95']].copy()\n",
    "max_idx = safety_95['safety_stock_95'].idxmax()\n",
    "min_idx = safety_95['safety_stock_95'].idxmin()\n",
    "max_row = safety_95.loc[max_idx]\n",
    "min_row = safety_95.loc[min_idx]\n",
    "avg_safety_95 = safety_95['safety_stock_95'].mean()\n",
    "\n",
    "print('Highest safety stock SKU (95% level):')\n",
    "print(f\"  SKU: {max_row['sku_number']}  |  Safety Stock: {max_row['safety_stock_95']:.2f}\")\n",
    "print('\\nLowest safety stock SKU (95% level):')\n",
    "print(f\"  SKU: {min_row['sku_number']}  |  Safety Stock: {min_row['safety_stock_95']:.2f}\")\n",
    "print('\\nAverage safety stock across all SKUs (95% level):')\n",
    "print(f\"  {avg_safety_95:.2f}\")\n",
    "\n",
    "max_row, min_row, avg_safety_95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f3469",
   "metadata": {},
   "source": [
    "### Side Quest — Empirical (Non‑Parametric) Safety Stock (95th Percentile)\n",
    "Use **observed** demand (no normality assumption):  \n",
    "`SS_empirical_95 = quantile_95(order_quantity) × √(avg_lead_time)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_stats = (\n",
    "    fg_mts.groupby('sku_number')\n",
    "    .agg(\n",
    "        q95_demand = ('order_quantity', lambda x: np.quantile(x, 0.95)),\n",
    "        avg_lead_time = ('lead_time', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "quantile_stats['safety_stock_empirical_95'] = (\n",
    "    quantile_stats['q95_demand'] * np.sqrt(quantile_stats['avg_lead_time'])\n",
    ")\n",
    "quantile_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d84f29",
   "metadata": {},
   "source": [
    "#### Quick Comparison (normal vs empirical)\n",
    "This table helps spot SKUs where the **normal-based** and **empirical** methods diverge the most.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = (\n",
    "    quantile_stats[['sku_number','safety_stock_empirical_95']]\n",
    "    .merge(sku_stats[['sku_number','safety_stock_95']], on='sku_number', how='inner')\n",
    "    .assign(diff=lambda d: d['safety_stock_empirical_95'] - d['safety_stock_95'])\n",
    "    .sort_values('diff', ascending=False)\n",
    ")\n",
    "\n",
    "# Show top 5 where empirical >> normal\n",
    "top5_empirical_over = compare.head(5)\n",
    "\n",
    "# Show top 5 where normal >> empirical\n",
    "top5_normal_over = compare.sort_values('diff').head(5)\n",
    "\n",
    "top5_empirical_over, top5_normal_over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f425441",
   "metadata": {},
   "source": [
    "## Conclusion (Milestone III)\n",
    "- Combined **cleaning & filtering** (A1) with **safety stock** (A2) in a single, reproducible notebook.\n",
    "- Cleaned dataset (consistent columns, dates, returns flagged, NaNs handled).  \n",
    "- Focused on **FG + MTS** SKUs and reported core counts and top/bottom tables.\n",
    "- Created per‑SKU demand/lead time summaries and computed safety stock at 75/90/95% service levels.  \n",
    "- Answered the 95% service level questions (largest, smallest, average).\n",
    "- The **Side Quest** used an empirical 95th percentile method (no normality assumption) and included a quick comparison to flag SKUs where the normal formula might over/under-estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80714fa1",
   "metadata": {},
   "source": [
    "## References\n",
    "- OM 620 class notes and slides (Dr. Majid Karimi)  \n",
    "- Pandas documentation (groupby, agg, quantile)  \n",
    "- NumPy documentation  \n",
    "- Collaboration/discussion with my classmate Chris  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
